import scrapy
import json


class ContentExtractorSpider(scrapy.Spider):
    name = "content_extractor"
    allowed_domains = ["dynodesoft.org"]

    def start_requests(self):
        # Load the links from the file generated by the link extractor spider
        with open("links.json", "r") as file:
            links = json.load(file)
            for link in links:
                yield scrapy.Request(
                    link["link"],
                    callback=self.parse_blog_page,
                    meta={"title": link["title"], "detail": link["detail"]},
                )

    def parse_blog_page(self, response):
        # Extract content from the individual blog page (from div.cen_header)
        content = response.css("div.cen_header").get()

        # Yield the result combining the main page data and individual content
        yield {
            "title": response.meta["title"],
            "detail": response.meta["detail"],
            "link": response.url,
            "content": content if content else "Content not found",
        }
